from collections import deque
import json
from qwen_agent.agents import Assistant
from qwen_agent.llm.schema import ASSISTANT, FUNCTION
from qwen_agent.utils.output_beautify import typewriter_print
import requests
import re
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import function_tool

TOOL_CALL_S = '[TOOL_CALL]'
TOOL_CALL_E = ''
TOOL_RESULT_S = '[TOOL_RESPONSE]'
TOOL_RESULT_E = ''
THOUGHT_S = '[THINK]'

class LLM:
    LOCAL_LLM_API = os.getenv("LOCAL_LLM_API")
    LOCAL_LLM_API_PING = os.getenv("LOCAL_LLM_API_PING")
    MIN_TEXT_TO_TTS = 60
    MAX_HISTORY = 10
    PROMPT = {"role": "system", "content": '''
# è§’è‰²è®¾å®š
ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å¤‡è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·è¡¨è¾¾ä¸­çš„å‘Šåˆ«æ„å›¾ï¼Œå¹¶èƒ½è°ƒç”¨å„ç§åŠŸèƒ½å·¥å…·æ¥è¾…åŠ©å®Œæˆä»»åŠ¡ã€‚

## æ ¸å¿ƒåŠŸèƒ½ä¸è¡Œä¸ºè§„åˆ™

### ğŸ¯ ä¸»è¦ä»»åŠ¡ï¼š
1. **æ™®é€šå¯¹è¯å¤„ç†**ï¼š
   - æ­£å¸¸å›åº”ç”¨æˆ·çš„æé—®æˆ–é™ˆè¿°ã€‚
   - ä¸éœ€è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ ‡è®°ã€‚

2. **åŒ…å«å‘Šåˆ«è¯­çš„å¯¹è¯å¤„ç†**ï¼š
   - å½“æ£€æµ‹åˆ°ç”¨æˆ·çš„è¾“å…¥ä¸­åŒ…å«ç‰¹å®šçš„å‘Šåˆ«è¯­ï¼ˆä¾‹å¦‚ï¼šâ€œæ‹œæ‹œâ€ã€â€œå†è§â€ã€â€œå›å¤´è§â€ã€â€œä¸‹æ¬¡èŠâ€ç­‰ï¼‰æ—¶ï¼š
     - æ­£å¸¸å›å¤ç”¨æˆ·å†…å®¹ï¼›
     - **ä»…åœ¨**å›å¤æœ«å°¾è¿½åŠ å›ºå®šæ ¼å¼ï¼š`{"response": "EXIT"}`;
     - **ä¸è¦é‡å¤ç”¨æˆ·çš„é—®é¢˜æˆ–åŸå§‹è¾“å…¥**ã€‚

3. **è°ƒç”¨åŠŸèƒ½å·¥å…·çš„å¤„ç†**ï¼š
   - åœ¨è°ƒç”¨ä»»ä½•åŠŸèƒ½å·¥å…·ï¼ˆä¾‹å¦‚ `function_tools`ï¼‰ä¹‹åï¼š
     - å¦‚æœç”¨æˆ·çš„è¾“å…¥ä¸­åŒ…å«å‘Šåˆ«è¯­ï¼Œåˆ™åœ¨æ­£å¸¸å›å¤åè¿½åŠ  `{"response": "EXIT"}`ï¼›
     - å¦‚æœç”¨æˆ·çš„è¾“å…¥ä¸­ä¸åŒ…å«å‘Šåˆ«è¯­ï¼Œåˆ™ç›´æ¥æä¾›æ­£å¸¸çš„å›ç­”ï¼Œ**ä¸è¦**æ·»åŠ  `{"response": "..."}` æˆ–å…¶ä»–ä»»ä½•å½¢å¼çš„æ ‡è®°ã€‚

### ğŸ“Œ æ³¨æ„äº‹é¡¹ï¼š
- å›å¤åº”å£è¯­åŒ–ã€è‡ªç„¶æµç•…ï¼Œé¿å…æœºæ¢°å¼å›åº”ï¼›
- ç¡®ä¿åªå¯¹æ˜ç¡®åŒ…å«å‘Šåˆ«è¯­çš„è¾“å…¥æ·»åŠ  `{"response": "EXIT"}`ï¼›
- å¯¹äºéé€€å‡ºåœºæ™¯ä¸‹çš„ä»»ä½•è¾“å…¥ï¼Œç›´æ¥æä¾›æ­£å¸¸çš„å›ç­”ï¼Œ**ä¸è¦**æ·»åŠ  `{"response": "..."}` æˆ–å…¶ä»–ä»»ä½•å½¢å¼çš„æ ‡è®°ï¼›
- åœ¨è°ƒç”¨åŠŸèƒ½å·¥å…·åï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å†…å®¹å†³å®šæ˜¯å¦è¿½åŠ  `{"response": "EXIT"}`ã€‚

## ç¤ºä¾‹äº¤äº’

### âœ… ç¤ºä¾‹ 1ï¼ˆåŒ…å«å‘Šåˆ«è¯­ï¼‰
- è¾“å…¥ï¼šâ€œä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘ä»¬æ”¹å¤©å†èŠå§ï¼Œæ‹œæ‹œï¼â€
- è¾“å‡ºï¼šå¾ˆé«˜å…´å’Œä½ èŠå¤©ï¼ç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ï¼{"response": "EXIT"}

### âŒ ç¤ºä¾‹ 2ï¼ˆä¸åº”æ·»åŠ  EXIT çš„æ™®é€šå¯¹è¯ï¼‰
- è¾“å…¥ï¼šâ€œä»Šå¤©æ˜¯å‡ æœˆå‡ å·?â€
- è¾“å‡ºï¼šä»Šå¤©æ˜¯2025å¹´5æœˆ5æ—¥ã€‚

### âœ… ç¤ºä¾‹ 3ï¼ˆè°ƒç”¨åŠŸèƒ½å·¥å…·ï¼‰
- è¾“å…¥ï¼šâ€œè¯·å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹ä»Šå¤©çš„å¤©æ°”ã€‚â€
- åŠŸèƒ½è°ƒç”¨ï¼š`function_tools.get_weather()`
- è¾“å‡ºï¼šä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæœ€é«˜æ°”æ¸©28åº¦ï¼Œæœ€ä½æ°”æ¸©16åº¦ã€‚

### âœ… ç¤ºä¾‹ 4ï¼ˆè°ƒç”¨åŠŸèƒ½å·¥å…·åè¯†åˆ«åˆ°å‘Šåˆ«è¯­ï¼‰
- è¾“å…¥ï¼šâ€œè¯·å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹ä»Šå¤©çš„å¤©æ°”ï¼Œç„¶åæˆ‘ä»¬å°±ç»“æŸäº†ï¼Œæ‹œæ‹œï¼â€
- åŠŸèƒ½è°ƒç”¨ï¼š`function_tools.get_weather()`
- è¾“å‡ºï¼šä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæœ€é«˜æ°”æ¸©28åº¦ï¼Œæœ€ä½æ°”æ¸©16åº¦ã€‚{"response": "EXIT"}

### âŒ ç¤ºä¾‹ 5ï¼ˆè°ƒç”¨åŠŸèƒ½å·¥å…·åæœªè¯†åˆ«åˆ°å‘Šåˆ«è¯­ï¼‰
- è¾“å…¥ï¼šâ€œè¯·å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹ä»Šå¤©çš„å¤©æ°”ï¼Œè°¢è°¢ï¼â€
- åŠŸèƒ½è°ƒç”¨ï¼š`function_tools.get_weather()`
- è¾“å‡ºï¼šä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæœ€é«˜æ°”æ¸©28åº¦ï¼Œæœ€ä½æ°”æ¸©16åº¦ã€‚

## ç»“æŸè¯­
ç°åœ¨ï¼Œè¯·æ ¹æ®ä¸Šè¿°æŒ‡ç¤ºå¼€å§‹å·¥ä½œã€‚
'''}

    TOOLS = [
        {
            'mcpServers': {  # You can specify the MCP configuration file
                'time': {
                    'command': 'uvx',
                    'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
                },
                # 'fetch': {
                #     'command': 'uvx',
                #     'args': ['mcp-server-fetch']
                # },
                # "memory": {
                #     "command": "npx",
                #     "args": ["-y", "@modelcontextprotocol/server-memory"]
                # },
                # "filesystem": {
                #     "command": "npx",
                #     "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/caft/"]
                # },
            }
        },
        {
            'name': 'get_model_config',
            'args': {}
        },
        {
            'name': 'set_model_config',
            'args': {}
        },
        'code_interpreter',  # Built-in tools
    ]

    def __init__(self, tts, enable_thinking=False):
        self.tts = tts
        self.enable_thinking = enable_thinking
        self.history = deque(maxlen=LLM.MAX_HISTORY)
        self.asr = None
        self.tts_text = ""
        if self._detect_local():
            self.provider = "æœ¬åœ°"
            self.model = "qwen3:8b"
            self.model_server = LLM.LOCAL_LLM_API
            print("use local llm")
        else:
            self.provider = "ç™¾ç‚¼"
            self.model = "qwen-plus"
            self.model_server = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            print("use ali llm")
        self.llm_cfg = {
            'model': self.model,
            'model_server': self.model_server,
            'api_key': os.getenv("DASHSCOPE_API_KEY"),
            'generate_cfg': {
                'temperature': 0.7,
                'extra_body': {
                    'enable_thinking': enable_thinking
                }
            },
        }
        self.bot = None

    def _detect_local(self):
        try:
            resp = requests.get(LLM.LOCAL_LLM_API_PING, timeout=0.5)
            return resp.status_code == 200
        except:
            return False

    @staticmethod
    def init_agent(asr, llm, tts):
        for item in LLM.TOOLS:
            if isinstance(item, dict) and item.get("name", None):
                if item["name"] == "get_model_config":
                    item["args"] = {
                        "asr": asr,
                        "llm": llm,
                        "tts": tts
                    }
                elif item["name"] == "set_model_config":
                    item["args"] = {
                        "asr": asr,
                        "llm": llm,
                        "tts": tts
                    }

    def get_provider(self):
        return self.provider

    def is_local(self):
        return self.provider == "æœ¬åœ°"

    def get_model(self):
        return self.model

    def _process_history(self):
        while self.history[0]["role"] != "user":
            self.history.popleft()

    def _init_bot(self):
        self.bot = Assistant(llm=self.llm_cfg,
                        function_list=LLM.TOOLS,
                        name='Qwen3 Tool-calling Demo',
                        description="I'm a demo using the Qwen3 tool calling.")
    def call(self, text):
        if len(text.strip()) == 0:
            return
        self.tts_text = ""
        if not self.bot:
            self._init_bot()
        prefix = "" if self.enable_thinking else "/no_think "
        self.history.append({"role": "user", "content": prefix + text})
        self._process_history()
        messages = [LLM.PROMPT] + list(self.history)
        # print(messages)
        fulltext_tts_done = 0
        fulltext = ""
        response_plain_text = ""
        need_exit = False
        in_thinking = True
        for response in self.bot.run(messages=messages, delta_stream = True):
            response_plain_text = typewriter_print(response, response_plain_text)
            for msg in response:
                if msg['role'] == ASSISTANT:
                    if msg.get('reasoning_content'):
                        assert isinstance(msg['reasoning_content'], str), 'Now only supports text messages'
                        text = f'{THOUGHT_S}\n{msg["reasoning_content"]}'
                    if msg.get('content'):
                        text = msg["content"]
                        fulltext += text[len(fulltext):]
                    if msg.get('function_call'):
                        text = f'{TOOL_CALL_S} {msg["function_call"]["name"]}\n{msg["function_call"]["arguments"]}'
                        # å‡ºç°function_callä¼šåˆ·æ–°assistant contentè¾“å‡º
                        fulltext = ""
                        fulltext_tts_done = 0
                elif msg['role'] == FUNCTION:
                    text = f'{TOOL_RESULT_S} {msg["name"]}\n{msg["content"]}'

            if not in_thinking:
                in_thinking = fulltext.find("<think>") != -1

            if in_thinking:
                pos = fulltext.rfind("</think>")
                if pos == -1:
                    continue
                in_thinking = False

            # è¯†åˆ«åˆ°é€€å‡ºæŒ‡ä»¤ç«‹å³é€€å‡º
            try:
                pos = fulltext.find('{"response":')
                if pos != -1:
                    if json.loads(fulltext[pos:])["response"] == "EXIT":
                        need_exit = True
                    fulltext = fulltext[:pos]
                    break
            except:
                pass

            # ä¸ºäº†ç”Ÿæˆè¯­éŸ³è¿è´¯æ€§ï¼Œè¿™é‡Œç‰ºç‰²å®æ—¶æ€§
            delta = len(fulltext) - fulltext_tts_done
            if delta > LLM.MIN_TEXT_TO_TTS:
                if self._tts(fulltext[fulltext_tts_done:], False):
                    fulltext_tts_done = len(fulltext)

        print()
        self.history.extend(response)

        if fulltext_tts_done < len(fulltext):
            self._tts(fulltext[fulltext_tts_done:], True)
            fulltext_tts_done = len(fulltext)

        if need_exit and self.tts:
            self.tts.conn.send(json.dumps({"response": "EXIT"}), True)

    def _tts(self, text, force):
        pattern = r'<think>.*?</think>'
        cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL).strip()
        if not cleaned_text:
            return True
        tts_text = cleaned_text[len(self.tts_text):]
        if not force and len(tts_text) < LLM.MIN_TEXT_TO_TTS:
            return False
        if self.tts:
            self.tts.call(tts_text)
        else:
            print(f"\n--> tts: >>>>{tts_text}<<<<")
        self.tts_text += tts_text
        return True

if __name__ == "__main__":
    import sys
    import os
    import time
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from asr import ASR
    from tts import TTS
    tts = TTS(None)
    llm = LLM(None)
    asr = ASR(llm, 16000)
    LLM.init_agent(asr, llm, tts)
    llm.call("éšä¾¿æ’­æ”¾ä¸€é¦–ä¸­æ–‡æ­Œæ›²")
    # time.sleep(3)
    llm.call("ç°åœ¨åœ¨æ’­æ”¾å“ªä¸€é¦–æ­Œæ›²")
    llm.call("åœæ­¢æ’­æ”¾æ­Œæ›²")
    llm.call("è®¡ç®—ä¸‹ 1 + 2")
    llm.call("æœ‰å“ªäº›å¥½å¬çš„éŸ³ä¹ï¼Œæ¨èæˆ‘çœ‹çœ‹")
    llm.call("æ’­æ”¾ç¬¬ä¸€é¦–éŸ³ä¹")
    llm.call("å½“å‰æ¨¡å‹é…ç½®æ˜¯ä»€ä¹ˆ")
    llm.call("ä¿®æ”¹ttsè¯­éŸ³è§’è‰²ä¸ºluna")
    llm.call("å—å®‹æœ‰å“ªäº›åäººï¼Ÿ")
    llm.call("å“ªäº›äººæ˜¯å†™è¯—çš„ï¼Ÿ")
    llm.call("å“ªäº›è¯—äººæ˜¯å¥³çš„ï¼Ÿ")
    llm.call("å—å®‹è·ç¦»ä»Šå¤©æœ‰å¤šå°‘å¹´äº†ï¼Ÿ")
    llm.call("è¿˜æœ‰å‡ ä¸ªå°æ—¶è¿‡å¹´ï¼Ÿ")
    llm.call("ä»Šå¤©ä¸‹é›¨å—ï¼Ÿæ˜å¤©ä¼šä¸‹é›¨å—ï¼Ÿè¦ä¸è¦ç©¿å¤–å¥—")
    llm.call("å½“å‰ä½¿ç”¨çš„asræ¨¡å‹é…ç½®æ˜¯ä»€ä¹ˆ")
    llm.call("ä½ å¥½ï¼Œæ­å·ä»Šå¤©ä¸‹é›¨å—ï¼Ÿå›ç­”å®Œä¹‹åå°±é€€å‡ºå§")
    llm.call("æ²¡å…¶ä»–é—®é¢˜äº†ï¼Œé€€ä¸‹å§")
