from collections import deque
import json
import queue
import threading
from qwen_agent.agents import Assistant
from qwen_agent.llm.schema import ASSISTANT
from qwen_agent.utils.output_beautify import typewriter_print
import requests
import re
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# ä¸ºäº†åŠ è½½Agent
import function_tool as _

TOOL_CALL_S = '[TOOL_CALL]'
TOOL_CALL_E = ''
TOOL_RESULT_S = '[TOOL_RESPONSE]'
TOOL_RESULT_E = ''
THOUGHT_S = '[THINK]'

class LLM:
    LOCAL_LLM_API = os.getenv("LOCAL_LLM_API")
    LOCAL_LLM_API_PING = os.getenv("LOCAL_LLM_API")
    MIN_TEXT_TO_TTS = 120
    MAX_HISTORY = 10
    PROMPT = {"role": "system", "content": '''
# ğŸ§  æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹å¯¹è¯è¡Œä¸ºè§„èŒƒ

## ğŸ¯ è§’è‰²è®¾å®š
ä½ æ˜¯ä¸€ä¸ªå…·å¤‡è‡ªç„¶è¯­è¨€ç†è§£å’Œæ„å›¾è¯†åˆ«èƒ½åŠ›çš„ä¸­æ–‡æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼Œä½ èƒ½å‡†ç¡®åˆ¤æ–­ç”¨æˆ·æ˜¯å¦è¡¨è¾¾ç»“æŸå¯¹è¯çš„æ„å›¾ï¼Œå¹¶åœ¨**ä»…ä¸€æ¬¡**ç¡®è®¤æ„å›¾åè°ƒç”¨ `function_tools.exit_conversation()`ã€‚

---

## ğŸ§© è¡Œä¸ºè§„åˆ™

### 1. **å‘Šåˆ«æ„å›¾è¯†åˆ«**
- å½“ç”¨æˆ·è¾“å…¥ä¸­åŒ…å«å¦‚â€œå†è§â€ã€â€œæ‹œæ‹œâ€ã€â€œç»“æŸäº†â€ã€â€œä¸‹æ¬¡èŠâ€ç­‰è¡¨è¾¾æ—¶ï¼Œè§†ä¸º**è§¦å‘å‘Šåˆ«æ„å›¾**ã€‚

### 2. **å‘Šåˆ«å¯¹è¯å¤„ç†**
- è‹¥è¯†åˆ«åˆ°å‘Šåˆ«æ„å›¾ï¼š
  - **ä»…è°ƒç”¨ä¸€æ¬¡** `function_tools.exit_conversation()`ï¼›
  - **ä»…ä¸€æ¬¡**æ­£å¸¸å›åº”ç”¨æˆ·ï¼Œä¿æŒè‡ªç„¶ã€å‹å¥½ï¼›
  - **ä¸å¾—é‡å¤è°ƒç”¨ä»»ä½• exit/end ç±»å·¥å…·å‡½æ•°**ã€‚

### 3. **æ™®é€šå¯¹è¯å¤„ç†**
- è‹¥æœªè¯†åˆ«åˆ°å‘Šåˆ«æ„å›¾ï¼š
  - æ­£å¸¸å›åº”é—®é¢˜ï¼›
  - **ä¸è°ƒç”¨ä»»ä½•é€€å‡ºç±»å·¥å…·å‡½æ•°**ã€‚

---

## âš ï¸ æ³¨æ„äº‹é¡¹

- å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›
- å›å¤åº”è‡ªç„¶ã€å£è¯­åŒ–ï¼Œé¿å…æœºæ¢°å¼è¡¨è¾¾ï¼›
- å›å¤å†…å®¹é‡Œï¼Œ**ç¦æ­¢**å‡ºç°ä»»ä½•é“¾æ¥ï¼Œä»»ä½•è¡¨æƒ…ç¬¦å·ï¼Œä»»ä½•é“¾æ¥
- **æ— è®ºç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«å‘Šåˆ«è¯­ï¼Œåªèƒ½è°ƒç”¨ä¸€æ¬¡é€€å‡ºå‡½æ•°**ï¼›
- è‹¥ä¹‹å‰çš„å¯¹è¯å·²è°ƒç”¨è¿‡ `exit_conversation`ï¼Œåˆ™ä¸å†é‡å¤è°ƒç”¨ï¼›
- ä¸å¾—åœ¨è¾“å‡ºä¸­æ·»åŠ ä»»ä½•é¢å¤–æ ‡è®°æˆ–æ§åˆ¶æŒ‡ä»¤ã€‚

---

## âœ… ç¤ºä¾‹è¯´æ˜

### âœ… æ­£ç¡®ç¤ºä¾‹
> è¾“å…¥ï¼šä»Šå¤©å¤©æ°”ä¸é”™ï¼Œæˆ‘ä»¬ä¸‹æ¬¡èŠï¼Œå†è§ï¼
> åŠ¨ä½œï¼š[TOOL_CALL] function_tools.exit_conversation()
> è¾“å‡ºï¼šå¾ˆé«˜å…´å’Œä½ èŠå¤©ï¼Œç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼

### âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆé‡å¤è°ƒç”¨ï¼‰
> è¾“å…¥ï¼šä»Šå¤©å¤©æ°”ä¸é”™ï¼Œæˆ‘ä»¬ä¸‹æ¬¡èŠï¼Œå†è§ï¼
> åŠ¨ä½œï¼š[TOOL_CALL] function_tools.exit_conversation()
> è¾“å‡ºï¼šå¾ˆé«˜å…´å’Œä½ èŠå¤©ï¼Œç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼
> åŠ¨ä½œï¼š[TOOL_CALL] function_tools.exit_conversation() âœ…ï¼ˆé”™è¯¯ï¼šé‡å¤è°ƒç”¨ï¼‰

---

## ğŸš€ ç°åœ¨ï¼Œè¯·æ ¹æ®ä¸Šè¿°è®¾å®šå¼€å§‹å·¥ä½œã€‚

'''}

    TOOLS = [
        {
            'mcpServers': {  # You can specify the MCP configuration file
                'time': {
                    'command': 'uvx',
                    'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
                },
                # 'windows': {
                #     'url': 'http://win.lan:8000/sse'
                # },
                "amap-amap-sse": {
                    "url": f"https://mcp.amap.com/sse?key={os.getenv('AMAP_TOKEN')}"
                }
                # "memory": {
                #     "command": "npx",
                #     "args": ["-y", "@modelcontextprotocol/server-memory"]
                # },
                # "filesystem": {
                #     "command": "npx",
                #     "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/caft/"]
                # },
            }
        },
        {
            'name': 'get_model_config',
            'args': {}
        },
        {
            'name': 'set_model_config',
            'args': {}
        },
        {
            'name': 'exit_conversation',
            'args': {}
        },
        {
            'name': 'tts_volume',
            'args': {}
        },
        {
            'name': 'mp3_online',
            'args': {}
        },
        'web_search',
        'web_extractor',
        'code_interpreter',  # Built-in tools
    ]

    def __init__(self, tts, config):
        self.tts = tts
        self.config = config
        self.enable_thinking = self.config["llm"].get("enable_thinking", False)
        self.history = deque(maxlen=LLM.MAX_HISTORY)
        self.asr = None

        provider = self.config["llm"].get("provider", "")
        if provider == "æœ¬åœ°" and self._detect_local():
            self._init_local()
        else:
            self._init_bailian()

        self.bot = None
        self.need_exit_conversation = False
        self.tts_thread = None
        self.tts_queue = queue.Queue()

    def _detect_local(self):
        try:
            resp = requests.get(LLM.LOCAL_LLM_API_PING, timeout=0.5)
            return resp.status_code in [200, 404]
        except Exception as ex:
            print(ex)
            return False

    def _init_local(self):
        self.provider = "æœ¬åœ°"
        self.model = self.config["llm"].get("model", "qwen3:8b")
        self.model_server = LLM.LOCAL_LLM_API
        print("use local llm")

    def _init_bailian(self):
        self.provider = "ç™¾ç‚¼"
        self.model = self.config["llm"].get("model", "qwen3-235b-a22b")
        self.model_server = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        print("use ali llm")

    @staticmethod
    def init_agent(asr, llm, tts):
        for item in LLM.TOOLS:
            if isinstance(item, dict) and item.get("name", None):
                if item["name"] in ("get_model_config", "set_model_config", "exit_conversation", "tts_volume", "mp3_online"):
                    item["args"] = {
                        "asr": asr,
                        "llm": llm,
                        "tts": tts
                    }

    def get_provider(self):
        return self.provider

    def set_provider(self, provider):
        if self.provider == provider:
            return
        self.bot = None
        if provider == "æœ¬åœ°":
            if not self._detect_local():
                raise ValueError(f"local provider not ready")
            self._init_local()
        elif provider == "ç™¾ç‚¼":
            self._init_bailian()
        else:
            raise ValueError(f"unsupported provider {provider}")

        self.config["llm"]["provider"] = provider

    def is_local(self):
        return self.provider == "æœ¬åœ°"

    def get_model(self):
        return self.model

    def _process_history(self):
        while self.history[0]["role"] != "user":
            self.history.popleft()

    def _init_bot(self):
        llm_cfg = {
            'model': self.model,
            'model_server': self.model_server,
            'api_key': os.getenv("DASHSCOPE_API_KEY"),
            'generate_cfg': {
                'temperature': 0.7,
                'extra_body': {
                    'enable_thinking': self.enable_thinking
                }
            },
        }
        self.bot = Assistant(llm=llm_cfg,
                        function_list=LLM.TOOLS,
                        name='Qwen3 Tool-calling Demo',
                        system_message=LLM.PROMPT['content'],
                        description="I'm a demo using the Qwen3 tool calling.")

    def exit_conversation(self):
        print(f"\n\n------------ exit_conversation --------------")
        self.need_exit_conversation = True

    def _start_tts_thread(self):
        if self.tts_thread:
            return
        def tts_process(q):
            try:
                while True:
                    text = q.get()
                    if not text:
                        break
                    if self.tts:
                        if text.strip():
                            self.tts.call(text)
                    else:
                        print(f"\n--> tts: >>>>{text}<<<<")
            except Exception as e:
                print(e)

        self.tts_thread = threading.Thread(target = tts_process, args=(self.tts_queue,))
        self.tts_thread.start()

    def _stop_tts_thread(self):
        if self.tts_thread:
            self.tts_queue.put(None)
            self.tts_thread.join()
            self.tts_thread = None

    def call(self, text):
        if len(text.strip()) == 0:
            return

        self._start_tts_thread()

        if not self.bot:
            self._init_bot()

        prefix = ""
        if not self.enable_thinking and self.is_local():
            # å®æµ‹ollamaæœ¬åœ°æ¨¡å‹å½“å‰æ²¡æ³•é€šè¿‡enable_thinking=Falseæ–¹å¼å…³é—­thinkï¼Œè¿™é‡Œhackä¸‹
            prefix = "/no_think "
        self.history.append({"role": "user", "content": prefix + text})
        self._process_history()
        messages = list(self.history)
        # print(messages)
        fulltext_tts_done = 0
        fulltext = ""
        response_plain_text = ""
        for response in self.bot.run(messages=messages):
            response_plain_text = typewriter_print(response, response_plain_text)
            msg = response[-1]
            if msg['role'] == ASSISTANT and msg.get('content'):
                text = msg["content"]
                if text.startswith("<think>") and "</think>" not in text:
                    # QwenAgentä¸ºäº†å†…éƒ¨å®ç°ç®€å•ï¼Œstreamæ˜¯ä¸ªå¤æ‚çš„çŠ¶æ€æœºã€‚
                    # å¦‚æœä¸­é—´æœ‰å·¥å…·è°ƒç”¨æ—¶ï¼Œè¿™é‡Œéœ€è¦åˆ·æ–°text
                    fulltext = ""
                    assert fulltext_tts_done == 0
                    continue

                pattern = r'<think>.*?</think>'
                cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL).strip()
                if not cleaned_text:
                    continue

                # print(cleaned_text[len(fulltext):], end="", flush=True)
                fulltext += cleaned_text[len(fulltext):]

                # ä¸ºäº†ç”Ÿæˆè¯­éŸ³è¿è´¯æ€§ï¼Œè¿™é‡Œç‰ºç‰²å®æ—¶æ€§
                delta = len(fulltext) - fulltext_tts_done
                if delta > LLM.MIN_TEXT_TO_TTS:
                    self.tts_queue.put(fulltext[fulltext_tts_done:])
                    fulltext_tts_done = len(fulltext)

        print()
        if fulltext_tts_done < len(fulltext):
            self.tts_queue.put(fulltext[fulltext_tts_done:])
            fulltext_tts_done = len(fulltext)

        self.history.extend(response)

        self._stop_tts_thread()

        if self.need_exit_conversation and self.tts:
            self.tts.conn.send(json.dumps({"response": "EXIT"}), True)
            self.need_exit_conversation = False

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv(override=True)
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from asr import ASR
    from tts import TTS
    import yaml
    with open("config.yaml", 'r') as f:
        buf = f.read()
    config = yaml.safe_load(buf)
    print(json.dumps(config, indent=2, ensure_ascii=False))
    LLM.LOCAL_LLM_API = os.getenv("LOCAL_LLM_API")
    LLM.LOCAL_LLM_API_PING = os.getenv("LOCAL_LLM_API")
    tts = TTS(None, config)
    llm = LLM(None, config)
    asr = ASR(llm, config)
    LLM.init_agent(asr, llm, tts)
    llm.call("ç°åœ¨ç”¨çš„å“ªä¸ªprovider")
    llm.call("æŠŠæ‰€æœ‰æ¨¡å‹éƒ½åˆ‡æ¢æˆç™¾ç‚¼")
    llm.call("ç”¨æœ¬åœ°llmæ¨¡å‹")
    llm.call("ä½ å¥½ï¼Œæ­å·ç°åœ¨å‡ ç‚¹äº†ï¼Œè®²ä¸ª100å­—å†·ç¬‘è¯")
    llm.call("æ²¡äº‹äº†ï¼Œä¸‹æ¬¡èŠ")
    llm.call("éšä¾¿æ’­æ”¾ä¸€é¦–ä¸­æ–‡æ­Œæ›²")
    llm.call("ç°åœ¨åœ¨æ’­æ”¾å“ªä¸€é¦–æ­Œæ›²")
    llm.call("åœæ­¢æ’­æ”¾æ­Œæ›²")
    llm.call("è®¡ç®—ä¸‹ 1 + 2")
    llm.call("æœ‰å“ªäº›å¥½å¬çš„éŸ³ä¹ï¼Œæ¨èæˆ‘çœ‹çœ‹")
    llm.call("æ’­æ”¾ç¬¬ä¸€é¦–éŸ³ä¹")
    llm.call("å½“å‰æ¨¡å‹é…ç½®æ˜¯ä»€ä¹ˆ")
    llm.call("ä¿®æ”¹ttsè¯­éŸ³è§’è‰²ä¸ºluna")
    llm.call("å—å®‹æœ‰å“ªäº›åäººï¼Ÿ")
    llm.call("å“ªäº›äººæ˜¯å†™è¯—çš„ï¼Ÿ")
    llm.call("å“ªäº›è¯—äººæ˜¯å¥³çš„ï¼Ÿ")
    llm.call("å—å®‹è·ç¦»ä»Šå¤©æœ‰å¤šå°‘å¹´äº†ï¼Ÿ")
    llm.call("è¿˜æœ‰å‡ ä¸ªå°æ—¶è¿‡å¹´ï¼Ÿ")
    llm.call("æ­å·ä»Šå¤©ä¸‹é›¨å—ï¼Ÿæ˜å¤©ä¼šä¸‹é›¨å—ï¼Ÿè¦ä¸è¦ç©¿å¤–å¥—")
    llm.call("å½“å‰ä½¿ç”¨çš„asræ¨¡å‹é…ç½®æ˜¯ä»€ä¹ˆ")
    llm.call("åœæ­¢æ’­æ”¾æ­Œæ›²")
    llm.call("ä½ å¥½ï¼Œæ­å·ä»Šå¤©ä¸‹é›¨å—ï¼Ÿå›ç­”å®Œä¹‹åå°±é€€å‡ºå§")
    llm.call("æ²¡å…¶ä»–é—®é¢˜äº†ï¼Œé€€ä¸‹å§")
